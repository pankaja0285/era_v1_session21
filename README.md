<h1> Bigram Language Model
<h2> Training GPT model from scratch </h2>
<p>
Libraries used:
- torch <br/>
- numpy <br/>
- pandas <br/>
</p>
<p>
Set up:
- Data source is set up by downloading from !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt <br/>
- Proceed to setting up training and validation data  <br/>
- Train with a BigramLanguage Model architecture using Attention mechanism <br/>
</p>
<br/>
<p>
File structure: <br/>
__ README.md <br/>
__ S21_GPTFromScratch_Andrej_v1.ipynb<br/>
</p>
<br/>
<h2> Steps: </h2>
<p>The notebook S21_GPTFromScratch_Andrej_v1.ipynb is self explanatory, all one needs to do is run the cells in sequence, irrespective of which environment you are running from Colab, or otherwise
</p>
