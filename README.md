## Bigram Language Model

### Training GPT model from scratch
<p>
Libraries used:
- torch
- numpy
- pandas
</p>
<p>
Set up:
- Data source is set up by downloading from !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt
- Proceed to setting up training and validation data 
- Train with a BigramLanguage Model architecture using Attention mechanism
</p>

<p>
File structure:
__ README.md <br/>
__ S21_GPTFromScratch_Andrej_v1.ipynb

</p>

### Steps:
<p>The notebook S21_GPTFromScratch_Andrej_v1.ipynb is self explanatory, all one needs to do is run the cells in sequence, irrespective of which environment you are running from Colab, or otherwise
</p>
