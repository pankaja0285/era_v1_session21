<h1> Bigram Language Model
<h2> Training GPT model from scratch </h2>
<p>
Libraries used:
- torch
- numpy
- pandas
</p>
<p>
Set up:
- Data source is set up by downloading from !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt
- Proceed to setting up training and validation data 
- Train with a BigramLanguage Model architecture using Attention mechanism
</p>
<br/>
<p>
File structure: <br/>
__ README.md <br/>
__ S21_GPTFromScratch_Andrej_v1.ipynb

</p>

<h2> Steps: </h2>
<p>The notebook S21_GPTFromScratch_Andrej_v1.ipynb is self explanatory, all one needs to do is run the cells in sequence, irrespective of which environment you are running from Colab, or otherwise
</p>
